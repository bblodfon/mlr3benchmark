#' @title Aggregated Benchmark Result Object
#' @description An R6 class for aggregated benchmark results.
#' @details An object of this class is not automatically generated by [mlr3::BenchmarkResult],
#' instead it should either be constructed from the results of `[mlr3::BenchmarkResult]$aggregate`,
#' or custom data can be entered as long as this at least includes the column names `learner_id`
#' (for models) and `task_id` (for datasets). Direct coercions from [mlr3::BenchmarkResult] are
#' available, which essentially wrap `$aggregate.`
#' @examples
#' library(mlr3)
#' task = tsks(c("boston_housing", "mtcars"))
#' learns = lrns(c("regr.featureless", "regr.rpart"))
#' bm = benchmark(benchmark_grid(task, learns, rsmp("cv", folds = 2)))
#'
#' # direct coercion
#' # default measure
#' as.BenchmarkAggr(bm)
#' # custom measure
#' as.BenchmarkAggr(bm, msr("regr.rmse"))
#'
#' # construct manually
#' BenchmarkAggr$new(bm$aggregate())
#'
#' # construct from non-mlr object
#' df = data.frame(task_id = rep(c("A", "B"), each = 5),
#'                 learner_id = paste0("L", 1:5),
#'                 RMSE = runif(10))
#' BenchmarkAggr$new(df)
#' @export
BenchmarkAggr = R6Class("BenchmarkAggr",
  public = list(
    #' @description
    #' Creates a new instance of this [R6][R6::R6Class] class.
    #' @param dt `(matrix(1))` \cr
    #' `matrix` like object coercable to [data.table::data.table][data.table], should
    #' include column names "task_id" and "learner_id", coerced to factors internally,
    #' and at least one measure (numeric).
    initialize = function(dt) {
      dt = as.data.table(dt)
      # at the very least should include task_id, learner_id, and one measure
      # FIXME - Need check that at least one measure included
      checkmate::assert(all(c("task_id", "learner_id") %in% colnames(dt)))
      dt$task_id = factor(dt$task_id)
      dt$learner_id = factor(dt$learner_id)
      private$.dt = dt
      invisible(self)
    },

    #' @description
    #' Prints the internal data via [data.table::print.data.table].
    #' @param ... Passed to [data.table::print.data.table].
    print = function(...) {
      print(private$.dt, ...)
    },

    #' @description Ranks the aggregated data given some measure.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param minimise `(logical(1))` \cr
    #' Should the measure be minimised? Default is `TRUE`.
    #' @param task `(character(1))` \cr
    #' If `NULL` then returns a matrix of ranks where columns are tasks and rows are
    #' learners, otherwise returns a one-column matrix of a specified task, should
    #' be in `$tasks`.
    rank_data = function(meas, minimise = TRUE, task = NULL) {
      meas = .check_meas(meas)
      df = subset(private$.dt, select = c("task_id", meas))
      lrns = self$learners
      nr = self$nlrns

      if (!minimize) {
        df[[meas]] = -df[[meas]]
      }

      if (!is.null(task)) {
        df = subset(private$.dt, task_id == task)
        rdf = matrix(data.table::frank(subset(df, select = meas)), ncol = 1)
        colnames(rdf) = task
      } else {
        tasks = self$tasks
        rdf = matrix(nrow = nr, ncol = self$ntasks)
        for(i in seq_along(tasks)) {
          rdf[,i] = data.table::frank(subset(df, task_id == tasks[[i]], select = meas))
        }
        colnames(rdf) = tasks
      }

      rownames(rdf) = lrns
      rdf
    },

    #' @description Computes Friedman test over all tasks, assumes datasets are independent.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. If no measure is provided
    #' then returns a matrix of tests for all measures.
    #' @param p.adjust.method `(character(1))` \cr
    #' Passed to [p.adjust] if `meas = NULL` for multiple testing correction. If `NULL`
    #' then no correction applied.
    friedman_test = function(meas = NULL, p.adjust.method = NULL) {

      if (self$nlrns < 2) {
        stop("At least two learners are required.")
      }

      if (self$ntasks < 2) {
        stop("At least two tasks are required")
      }

      if (!is.null(meas)) {
        return(friedman.test(as.formula(paste0(meas, " ~ learner_id | task_id", sep = "")),
                      data = private$.dt))
      } else {
        x = sapply(self$measures, function(x)
          friedman.test(as.formula(paste0(x, " ~ learner_id | task_id", sep = "")),
                       data = private$.dt))
        x = data.frame(t(x[1:3, ]))
        colnames(x) = c("X2", "df", "p.value")
        rownames(x) = self$measures

        if (!is.null(p.adjust.method)) {
          x$p.adj.value = p.adjust(x$p.value, p.adjust.method)
          x$p.signif = ifelse(x$p.adj.value <= 0.001, "***",
                              ifelse(x$p.adj.value <= 0.01, "**",
                                     ifelse(x$p.adj.value <= 0.05, "*",
                                            ifelse(x$p.adj.value <= 0.1, ".", " "))))
        } else {
          x$p.signif = ifelse(x$p.value <= 0.001, "***",
                              ifelse(x$p.value <= 0.01, "**",
                                     ifelse(x$p.value <= 0.05, "*",
                                            ifelse(x$p.value <= 0.1, ".", " "))))
        }
        return(x)
      }

    },

    #' @description Posthoc Friedman Nemenyi tests. Computed with
    #' [PMCMR::posthoc.friedman.nemenyi.test]. If global `$friedman_test` is non-significant then
    #' this is returned and no post-hocs computed. Also returns critical difference
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param p.value `(numeric(1))` \cr
    #' p.value for which the global test will be considered significant.
    friedman_posthoc = function(meas = NULL, p.value = 0.05) {

      meas = .check_meas(meas)
      checkmate::assertNumeric(p.value, lower = 0, upper = 1, len = 1)
      f.test = self$friedman_test(meas)

      if (!is.na(f.test$p.value)) {
        f.rejnull = f.test$p.value < p.value
        if (!f.rejnull) {
          warning("Cannot reject null hypothesis of overall Friedman test,
             returning overall Friedman test.")
        }
      } else {
        f.rejnull = FALSE
        warning("P-value not computable. Learner performances might be exactly equal.")
      }

      if (f.rejnull) {
        form = as.formula(paste0(meas, " ~ learner_id | task_id", sep = ""))
        nem.test = PMCMR::posthoc.friedman.nemenyi.test(form, data = private$.dt)
        nem.test$f.rejnull = f.rejnull
        return(nem.test)
      } else {
        f.test$f.rejnull = f.rejnull
        return(f.test)
      }
    },

    #' @description Generate critical differences for a critical difference diagram.
    #' Primarily should be used internally by [autoplot.BenchmarkAggr].
    #' @details Copied from mlr:
    #' Bonferroni-Dunn usually yields higher power than Nemenyi as it does not
    #' compare all algorithms to each other, but all algorithms to a
    #' `baseline` instead.
    #' Critical differences calculated as:
    #' \deqn{CD = q_{\alpha} \sqrt{\left(\frac{k(k+1)}{6N}\right)}}{CD = q_alpha sqrt(k(k+1)/(6N))} \cr # nolint
    #' Where \eqn{q_\alpha} is based on the studentized range statistic.
    #' See references for details.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param minimise `(logical(1))` \cr
    #' Should the measure be minimised? Default is `TRUE`.
    #' @param p.value `(numeric(1))` \cr
    #' p.value for which the global test will be considered significant.
    #' @param baseline `(character(1))` \cr
    #' For `test = "bd"` a baseline learner to compare the other learners to,
    #' should be in `$learners`.
    #' @param test (`character(1))`) \cr
    #' Should critical differences be computed from Bonferroni-Dunn (`bd`) or
    #' Nemenyi (`nemenyi`) tests? Default is Bonferroni-Dunn.
    #' @references Janez Demsar, Statistical Comparisons of Classifiers over
    #' Multiple Data Sets, JMLR, 2006
    crit_differences = function(meas = NULL, minimise = TRUE, p.value = 0.05, baseline = NULL,
                                test = c("bd", "nemenyi")) {

      meas = .check_meas(meas)
      test = match.arg(test)
      checkmate::assertNumeric(p.value, lower = 0, upper = 1, len = 1)

      # Get Rankmatrix, transpose and get mean ranks
      mean.rank = rowMeans(self$rank_data(meas))
      # Gather Info for plotting the descriptive part.
      df = data.frame(mean.rank,
                      learner_id = names(mean.rank),
                      rank = rank(mean.rank, ties.method = "average"))
      # Orientation of descriptive lines yend(=y-value of horizontal line)
      right = df$rank > median(df$rank)
      # Better learners are ranked ascending
      df$yend[!right] = rank(df$rank[!right], ties.method = "first") - 0.5
      # Worse learners ranked descending
      df$yend[right] = rank(-df$rank[right], ties.method = "first") - 0.5
      # Better half of learner have lines to left / others right.
      df$xend = ifelse(!right, 0L, max(df$rank) + 1L)
      # Save orientation, can be used for vjust of text later on
      df$right = as.numeric(right)
      df$short_name = self$learners

      # Get a baseline
      if (is.null(baseline)) {
        baseline = as.character(df$learner_id[which.min(df$rank)])
      } else {
        checkmate::assert_choice(baseline, self$learners)
      }

      # Perform nemenyi test
      nem.test = self$friedman_posthoc(meas, p.value)
      # calculate critical difference(s)
      if (test == "nemenyi") {
        cd = (qtukey(1 - p.value, self$nlrns, 1e+06) / sqrt(2L)) *
          sqrt(self$nlrns * (self$nlrns + 1L) / (6L * self$ntasks))
      } else {
        cd = (qtukey(1L - (p.value / (self$nlrns - 1L)), 2L, 1e+06) / sqrt(2L)) *
          sqrt(self$nlrns * (self$nlrns + 1L) / (6L * self$ntasks))
      }

      # Store Info for plotting the critical differences
      cd.info = list("test" = test,
                     "cd" = cd,
                     "x" = df$mean.rank[df$learner_id == baseline],
                     "y" = 0.1)

      # Create data for connecting bars (only nemenyi test)
      if (test == "nemenyi") {
        sub = sort(df$mean.rank)
        # Compute a matrix of all possible bars
        mat = apply(t(outer(sub, sub, `-`)), c(1, 2),
                    FUN = function(x) ifelse(x > 0 && x < cd.info$cd, x, 0))
        # Get start and end point of all possible bars
        xstart = round(apply(mat + sub, 1, min), 3)
        xend = round(apply(mat + sub, 1, max), 3)
        nem.df = data.table(xstart, xend, "diff" = xend - xstart)
        # For each unique endpoint of a bar keep only the longest bar
        nem.df = nem.df[, .SD[which.max(.SD$diff)], by = "xend"]
        # Take only bars with length > 0
        nem.df = nem.df[nem.df$xend - nem.df$xstart > 0, ]
        # Y-value for bars is between 0.1 and 0..35 hardcoded
        # Descriptive lines for learners start at 0.5, 1.5, ...
        nem.df$y = seq(from = 0.1, to = 0.35, length.out = dim(nem.df)[1])
        cd.info$nemenyi.data = as.data.frame(nem.df)
      }

      list("data" = df,
           "cd.info" = cd.info,
           "friedman.nemenyi.test" = nem.test,
           "baseline" = baseline,
           "p.value" = p.value)
    }
  ),

  active = list(
    #' @field data `([data.table::data.table])` \cr Aggregated data.
    data = function() private$.dt,
    #' @field learners `(character())` \cr Unique learner names.
    learners = function() as.character(unique(private$.dt$learner_id)),
    #' @field tasks `(character())` \cr Unique task names.
    tasks = function() as.character(unique(private$.dt$task_id)),
    #' @field measures `(character())` \cr Unique measure names.
    measures = function() {
      as.character(setdiff(colnames(private$.dt),
              c("nr", "task", "task_id", "learner", "learner_id", "resampling", "resampling_id",
                "iteration", "prediction", "resample_result", "iters")))
    },
    #' @field nlrns `(integers())` \cr Number of learners.
    nlrns = function() length(self$learners),
    #' @field ntasks `(integers())` \cr Number of tasks.
    ntasks = function() length(self$tasks),
    #' @field nmeas `(integers())` \cr Number of measures.
    nmeas = function() length(self$measures)
  ),

  private = list(
    .dt = data.table()
  )
)

.plot_crittdiff = function(obj, meas, ...) {
  obj = obj$crit_differences(meas, ...)

  # Plot descritptive lines and learner names
  p = ggplot(obj$data)
  # Point at mean rank
  p = p + geom_point(aes_string("mean.rank", 0, colour = "learner_id"), size = 3)
  # Horizontal descriptive bar
  p = p + geom_segment(aes_string("mean.rank", 0, xend = "mean.rank", yend = "yend",
                                  color = "learner_id"), size = 1)
  # Vertical descriptive bar
  p = p + geom_segment(aes_string("mean.rank", "yend", xend = "xend",
                                  yend = "yend", color = "learner_id"), size = 1)
  # Plot Learner name
  p = p + geom_text(aes_string("xend", "yend", label = "learner_id", color = "learner_id",
                               hjust = "right"), vjust = -1)

  p = p + xlab("Average Rank")
  # Change appearance
  p = p + scale_x_continuous(breaks = c(0:max(obj$data$xend)))
  p = p + theme(axis.text.y = element_blank(),
                axis.ticks.y = element_blank(),
                axis.title.y = element_blank(),
                legend.position = "none",
                panel.background = element_blank(),
                panel.border = element_blank(),
                axis.line = element_line(size = 1),
                axis.line.y = element_blank(),
                panel.grid.major = element_blank(),
                plot.background = element_blank())

  # Write some values into shorter names as they are used numerous times.
  cd.x = obj$cd.info$x
  cd.y = obj$cd.info$y
  cd = obj$cd.info$cd

  # Plot the critical difference bars
  if (obj$cd.info$test == "bd") {
    if (!is.null(baseline)) {
      checkmate::assert_choice(baseline, as.character(obj$data$learner_id))
      cd.x = obj$data$mean.rank[obj$data$learner_id == baseline]
    }
    # Add horizontal bar arround baseline
    p = p + annotate("segment", x = cd.x + cd, xend = cd.x - cd, y = cd.y, yend = cd.y,
                     alpha = 0.5, color = "darkgrey", size = 2)
    # Add intervall limiting bar's
    p = p + annotate("segment", x = cd.x + cd, xend = cd.x + cd, y = cd.y - 0.05,
                     yend = cd.y + 0.05, color = "darkgrey", size = 1)
    p = p + annotate("segment", x = cd.x - cd, xend = cd.x - cd, y = cd.y - 0.05,
                     yend = cd.y + 0.05, color = "darkgrey", size = 1)
    # Add point at learner
    p = p + annotate("point", x = cd.x, y = cd.y, alpha = 0.5)
    # Add critical difference text
    p = p + annotate("text", label = paste("Critical Difference =", round(cd, 2), sep = " "),
                     x = cd.x, y = cd.y + 0.05)
  } else {
    nemenyi.data = obj$cd.info$nemenyi.data
    if (!(nrow(nemenyi.data) == 0L)) {
      # Add connecting bars
      p = p + geom_segment(aes_string("xstart", "y", xend = "xend", yend = "y"),
                           data = nemenyi.data, size = 2, color = "dimgrey", alpha = 0.9)
      # Add text (descriptive)
      p = p + annotate("text",
                       label = stri_paste("Critical Difference =", round(cd, 2), sep = " "),
                       y = max(obj$data$yend) + .1, x = mean(obj$data$mean.rank))
      # Add bar (descriptive)
      p = p + annotate("segment",
                       x = mean(obj$data$mean.rank) - 0.5 * cd,
                       xend = mean(obj$data$mean.rank) + 0.5 * cd,
                       y = max(obj$data$yend) + .2,
                       yend = max(obj$data$yend) + .2,
                       size = 2L)
    } else {
      message("No connecting bars to plot!")
    }
  }
  return(p)
}
#' @title Plots for BenchmarkAggr
#'
#' @description
#' Generates plots for [BenchmarkAggr], depending on argument `type`:
#'
#' * `"mean"` (default): Assumes there are at least two independent tasks. Plots the sample mean
#' of the measure for all learners with error bars computed with the standard error of the mean.
#' * `"cd"`: Critical difference plots (Demsar, 2006), uses the `$crit_differences` method from
#' [BenchmarkAggr]. If a baseline is selected for the Bonferroni-Dunn test, the critical difference
#' interval will be positioned around the baseline. If not, the best performing algorithm will be
#' chosen as baseline. Learners are drawn on the y-axis according to their average rank.
#' For `test = "nemenyi"` a bar is drawn, connecting all groups of not
#' significantly different learners. For `test = "bd"` an interval is drawn around the algorithm
#' selected as a baseline and any learner within this interval is not significantly different
#' from the baseline.
#'
#' @param obj [BenchmarkAggr]
#' @param type `(character(1))` \cr Type of plot, see description.
#' @param meas `(character(1))` \cr Measure to plot, should be in `obj$measures`, can be `NULL` if
#' only one measure is in `obj`.
#' @param level `(numeric(1))` \cr Confidence level for error bars for `type = "mean"`
#' @param ... `ANY` \cr Additional arguments passed to `[BenchmarkAggr]$crit_differences`.
#'
#' @references Janez Demsar, Statistical Comparisons of Classifiers over
#' Multiple Data Sets, JMLR, 2006
#'
#' @export
autoplot.BenchmarkAggr = function(obj, type = c("mean", "cd"), meas = NULL, level = 0.95, ...) {

  type = match.arg(type)

  meas = .check_meas(meas)

  if (type == "cd") {
    .plot_crittdiff(obj, pretty.names, ...)
  } else {
    if (obj$ntasks < 2) {
      stop("At least two tasks required.")
    }
    loss = aggregate(as.formula(paste0(meas, " ~ learner_id")), obj$data, mean)
    se = aggregate(as.formula(paste0(meas, " ~ learner_id")), obj$data, sd)[, 2]/sqrt(obj$ntasks)
    loss$lower = loss[, meas] - se * qnorm(1 - (1 - level)/2)
    loss$upper = loss[, meas] + se * qnorm(1 - (1 - level)/2)
    ggplot(data = loss, aes_string(x = "learner_id", y = meas)) +
      geom_errorbar(aes(ymin = lower, ymax = upper),
                    width = .5) +
      geom_point()
  }

}

#' @export
as.BenchmarkAggr = function(obj, ...) UseMethod("as.BenchmarkAggr", obj)
#' @export
as.BenchmarkAggr.default = function(obj, ...) BenchmarkAggr$new(obj)
#' @export
as.BenchmarkAggr.BenchmarkResult = function(obj, ...) BenchmarkAggr$new(obj$aggregate(...))

.check_meas = function(obj, meas) {
  if (is.null(meas)) {
    if (obj$nmeas > 1) {
      stop("Multiple measures available but `meas` is NULL. Please specify a measure.")
    } else {
      return(obj$measures)
    }
  }
}
