#' @title Aggregated Benchmark Result Object
#' @description An R6 class for aggregated benchmark results.
#' @details An object of this class is not automatically generated by [mlr3::BenchmarkResult],
#' instead it should either be constructed from the results of `[mlr3::BenchmarkResult]$aggregate`,
#' or custom data can be entered as long as this at least includes the column names `learner_id`
#' (for models) and `task_id` (for datasets). Direct coercions from [mlr3::BenchmarkResult] are
#' available, which essentially wrap `$aggregate.`
#' @examples
#' library(mlr3)
#' task = tsks(c("boston_housing", "mtcars"))
#' learns = lrns(c("regr.featureless", "regr.rpart"))
#' bm = benchmark(benchmark_grid(task, learns, rsmp("cv", folds = 2)))
#'
#' # direct coercion
#' # default measure
#' as.BenchmarkAggr(bm)
#' # custom measure
#' as.BenchmarkAggr(bm, msr("regr.rmse"))
#'
#' # construct manually
#' BenchmarkAggr$new(bm$aggregate())
#'
#' # construct from non-mlr object
#' df = data.frame(task_id = rep(c("A", "B"), each = 5),
#'                 learner_id = paste0("L", 1:5),
#'                 RMSE = runif(10))
#' BenchmarkAggr$new(df)
#' @export
BenchmarkAggr = R6Class("BenchmarkAggr",
  public = list(
    #' @description
    #' Creates a new instance of this [R6][R6::R6Class] class.
    #' @param dt `(matrix(1))` \cr
    #' `matrix` like object coercable to [data.table::data.table][data.table], should
    #' include column names "task_id" and "learner_id", coerced to factors internally,
    #' and at least one measure (numeric).
    initialize = function(dt) {
      dt = as.data.table(dt)

      # at the very least should include task_id, learner_id, and one measure
      checkmate::assert(all(c("task_id", "learner_id") %in% colnames(dt)))
      dt$task_id = factor(checkmate::assert_character(dt$task_id))
      dt$learner_id = factor(checkmate::assert_character(dt$learner_id))

      # TODO - The line below could be removed if there is a use for this extra data,
      # for now there isn't in this object.
      dt = subset(dt, select = setdiff(colnames(dt),
                            c("nr", "task", "learner", "resampling", "resampling_id",
                              "iteration", "prediction", "resample_result", "iters")))

      if (ncol(dt) < 3) {
        stop("At least one measure must be included in `dt`.")
      }

      # check measures are numeric
      sapply(dt[, setdiff(colnames(dt), c("task_id", "learner_id")), with = FALSE], assert_numeric)

      private$.dt = dt
      invisible(self)
    },

    #' @description
    #' Prints the internal data via [data.table::print.data.table].
    #' @param ... Passed to [data.table::print.data.table].
    print = function(...) {
      catf("<BenchmarkAggr> of %i %s with %i %s, %i %s and %i %s",
           self$nrow, ifelse(self$nrow == 1, "row", "rows"),
           self$ntasks, ifelse(self$ntasks == 1, "task", "tasks"),
           self$nlrns, ifelse(self$nlrns == 1, "learner", "learners"),
           self$nmeas, ifelse(self$nmeas == 1, "measure", "measures"))
      print(private$.dt, ...)
    },

    #' @description Ranks the aggregated data given some measure.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param minimize `(logical(1))` \cr
    #' Should the measure be minimized? Default is `TRUE`.
    #' @param task `(character(1))` \cr
    #' If `NULL` then returns a matrix of ranks where columns are tasks and rows are
    #' learners, otherwise returns a one-column matrix of a specified task, should
    #' be in `$tasks`.
    rank_data = function(meas, minimize = TRUE, task = NULL) {
      meas = .check_meas(self, meas)
      df = subset(private$.dt, select = c("task_id", meas))
      lrns = self$learners
      nr = self$nlrns

      if (!minimize) {
        df[[meas]] = -df[[meas]]
      }

      if (!is.null(task)) {
        df = subset(df, task_id == task)
        rdf = matrix(data.table::frank(subset(df, select = meas)), ncol = 1)
        colnames(rdf) = task
      } else {
        tasks = self$tasks
        rdf = matrix(nrow = nr, ncol = self$ntasks)
        for (i in seq_along(tasks)) {
          rdf[, i] = data.table::frank(subset(df, task_id == tasks[[i]], select = meas))
        }
        colnames(rdf) = tasks
      }

      rownames(rdf) = lrns
      rdf
    },

    #' @description Computes Friedman test over all tasks, assumes datasets are independent.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. If no measure is provided
    #' then returns a matrix of tests for all measures.
    #' @param p.adjust.method `(character(1))` \cr
    #' Passed to [p.adjust] if `meas = NULL` for multiple testing correction. If `NULL`
    #' then no correction applied.
    friedman_test = function(meas = NULL, p.adjust.method = NULL) { # nolint

      if (self$nlrns < 2) {
        stop("At least two learners are required.")
      }

      if (self$ntasks < 2) {
        stop("At least two tasks are required")
      }

      if (!is.null(meas)) {
        return(friedman.test(as.formula(paste0(meas, " ~ learner_id | task_id", sep = "")),
                      data = private$.dt))
      } else {
        x = sapply(self$measures, function(x)
          friedman.test(as.formula(paste0(x, " ~ learner_id | task_id", sep = "")),
                       data = private$.dt))
        x = data.frame(t(x[1:3, ]))
        colnames(x) = c("X2", "df", "p.value")
        rownames(x) = self$measures

        if (!is.null(p.adjust.method)) {
          x$p.adj.value = p.adjust(x$p.value, p.adjust.method) # nolint
          x$p.signif = ifelse(x$p.adj.value <= 0.001, "***", # nolint
                              ifelse(x$p.adj.value <= 0.01, "**",
                                     ifelse(x$p.adj.value <= 0.05, "*",
                                            ifelse(x$p.adj.value <= 0.1, ".", " "))))
        } else {
          x$p.signif = ifelse(x$p.value <= 0.001, "***", # nolint
                              ifelse(x$p.value <= 0.01, "**",
                                     ifelse(x$p.value <= 0.05, "*",
                                            ifelse(x$p.value <= 0.1, ".", " "))))
        }
        return(x)
      }

    },

    #' @description Posthoc Friedman Nemenyi tests. Computed with
    #' [PMCMR::posthoc.friedman.nemenyi.test]. If global `$friedman_test` is non-significant then
    #' this is returned and no post-hocs computed. Also returns critical difference
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param p.value `(numeric(1))` \cr
    #' p.value for which the global test will be considered significant.
    friedman_posthoc = function(meas = NULL, p.value = 0.05) { # nolint

      meas = .check_meas(self, meas)
      checkmate::assertNumeric(p.value, lower = 0, upper = 1, len = 1)
      f.test = self$friedman_test(meas) # nolint

      if (!is.na(f.test$p.value)) {
        f.rejnull = f.test$p.value < p.value # nolint
        if (!f.rejnull) {
          warning("Cannot reject null hypothesis of overall Friedman test,
             returning overall Friedman test.")
        }
      } else {
        f.rejnull = FALSE # nolint
        warning("P-value not computable. Learner performances might be exactly equal.")
      }

      if (f.rejnull) {
        form = as.formula(paste0(meas, " ~ learner_id | task_id", sep = ""))
        nem.test = PMCMR::posthoc.friedman.nemenyi.test(form, data = private$.dt) # nolint
        nem.test$f.rejnull = f.rejnull # nolint
        return(nem.test)
      } else {
        f.test$f.rejnull = f.rejnull # nolint
        return(f.test)
      }
    },

    #' @description Generate critical differences for a critical difference diagram.
    #' Primarily should be used internally by [autoplot.BenchmarkAggr].
    #' FIXME - CONSIDER MOVING TO PRIVATE HELPER FUNCTION - PROBABLY NOT USEFUL IN ISOLATION
    #' @details Copied from mlr:
    #' Bonferroni-Dunn usually yields higher power than Nemenyi as it does not
    #' compare all algorithms to each other, but all algorithms to a
    #' `baseline` instead.
    #' Critical differences calculated as:
    #' \deqn{CD = q_{\alpha} \sqrt{\left(\frac{k(k+1)}{6N}\right)}}{CD = q_alpha sqrt(k(k+1)/(6N))} \cr # nolint
    #' Where \eqn{q_\alpha} is based on the studentized range statistic.
    #' See references for details.
    #' @param meas `(character(1))` \cr
    #' Measure to rank the data against, should be in `$measures`. Can be `NULL` if only one measure
    #' in data.
    #' @param minimize `(logical(1))` \cr
    #' Should the measure be minimized? Default is `TRUE`.
    #' @param p.value `(numeric(1))` \cr
    #' p.value for which the global test will be considered significant.
    #' @param baseline `(character(1))` \cr
    #' For `test = "bd"` a baseline learner to compare the other learners to,
    #' should be in `$learners`.
    #' @param test (`character(1))`) \cr
    #' Should critical differences be computed from Bonferroni-Dunn (`bd`) or
    #' Nemenyi (`nemenyi`) tests? Default is Bonferroni-Dunn.
    #' @references Janez Demsar, Statistical Comparisons of Classifiers over
    #' Multiple Data Sets, JMLR, 2006
    crit_differences = function(meas = NULL, minimize = TRUE, p.value = 0.05, baseline = NULL, # nolint
                                test = c("bd", "nemenyi")) {

      meas = .check_meas(self, meas)
      test = match.arg(test)
      checkmate::assertNumeric(p.value, lower = 0, upper = 1, len = 1)

      # Get Rankmatrix, transpose and get mean ranks
      mean.rank = rowMeans(self$rank_data(meas, minimize = minimize))
      # Gather Info for plotting the descriptive part.
      df = data.frame(mean.rank,
                      learner_id = names(mean.rank),
                      rank = rank(mean.rank, ties.method = "average"))
      # Orientation of descriptive lines yend(=y-value of horizontal line)
      right = df$rank > median(df$rank)
      # Better learners are ranked ascending
      df$yend[!right] = rank(df$rank[!right], ties.method = "first") - 0.5
      # Worse learners ranked descending
      df$yend[right] = rank(-df$rank[right], ties.method = "first") - 0.5
      # Better half of learner have lines to left / others right.
      df$xend = ifelse(!right, 0L, max(df$rank) + 1L)
      # Save orientation, can be used for vjust of text later on
      df$right = as.numeric(right)
      df$short_name = self$learners

      # Get a baseline
      if (is.null(baseline)) {
        baseline = as.character(df$learner_id[which.min(df$rank)])
      } else {
        checkmate::assert_choice(baseline, self$learners)
      }

      # Perform nemenyi test
      nem.test = self$friedman_posthoc(meas, p.value) # nolint
      # calculate critical difference(s)
      if (test == "nemenyi") {
        cd = (qtukey(1 - p.value, self$nlrns, 1e+06) / sqrt(2L)) *
          sqrt(self$nlrns * (self$nlrns + 1L) / (6L * self$ntasks))
      } else {
        cd = (qtukey(1L - (p.value / (self$nlrns - 1L)), 2L, 1e+06) / sqrt(2L)) *
          sqrt(self$nlrns * (self$nlrns + 1L) / (6L * self$ntasks))
      }

      # Store Info for plotting the critical differences
      cd.info = list("test" = test, # nolint
                     "cd" = cd,
                     "x" = df$mean.rank[df$learner_id == baseline],
                     "y" = 0.1)

      # Create data for connecting bars (only nemenyi test)
      if (test == "nemenyi") {
        sub = sort(df$mean.rank)
        # Compute a matrix of all possible bars
        mat = apply(t(outer(sub, sub, `-`)), c(1, 2),
                    FUN = function(x) ifelse(x > 0 && x < cd.info$cd, x, 0))
        # Get start and end point of all possible bars
        xstart = round(apply(mat + sub, 1, min), 3)
        xend = round(apply(mat + sub, 1, max), 3)
        nem.df = data.table(xstart, xend, "diff" = xend - xstart) # nolint
        # For each unique endpoint of a bar keep only the longest bar
        nem.df = nem.df[, .SD[which.max(.SD$diff)], by = "xend"] # nolint
        # Take only bars with length > 0
        nem.df = nem.df[nem.df$xend - nem.df$xstart > 0, ] # nolint
        # Y-value for bars is between 0.1 and 0..35 hardcoded
        # Descriptive lines for learners start at 0.5, 1.5, ...
        nem.df$y = seq(from = 0.1, by = 0.2, length.out = dim(nem.df)[1])
        cd.info$nemenyi.data = as.data.frame(nem.df) # nolint
      }

      list("data" = df,
           "cd.info" = cd.info,
           "friedman.nemenyi.test" = nem.test,
           "baseline" = baseline,
           "p.value" = p.value)
    }
  ),

  active = list(
    #' @field data `([data.table::data.table])` \cr Aggregated data.
    data = function() private$.dt,
    #' @field learners `(character())` \cr Unique learner names.
    learners = function() as.character(unique(private$.dt$learner_id)),
    #' @field tasks `(character())` \cr Unique task names.
    tasks = function() as.character(unique(private$.dt$task_id)),
    #' @field measures `(character())` \cr Unique measure names.
    measures = function() {
      setdiff(colnames(private$.dt), c("task_id", "learner_id"))
    },
    #' @field nlrns `(integers())` \cr Number of learners.
    nlrns = function() length(self$learners),
    #' @field ntasks `(integers())` \cr Number of tasks.
    ntasks = function() length(self$tasks),
    #' @field nmeas `(integers())` \cr Number of measures.
    nmeas = function() length(self$measures),
    #' @field nrow `(integers())` \cr Number of rows.
    nrow = function() nrow(self$data)
  ),

  private = list(
    .dt = data.table()
  )
)

.plot_crittdiff = function(obj, meas, p.value, ...) { # nolint
  obj = obj$crit_differences(meas = meas, p.value = p.value, ...)

  # Plot descriptive lines and learner names
  p = ggplot(obj$data)
  # Point at mean rank
  p = p + geom_point(aes_string("mean.rank", 0, colour = "learner_id"), size = 3)
  # Horizontal descriptive bar
  p = p + geom_segment(aes_string("mean.rank", 0, xend = "mean.rank", yend = "yend",
                                  color = "learner_id"), size = 1)
  # Vertical descriptive bar
  p = p + geom_segment(aes_string("mean.rank", "yend", xend = "xend",
                                  yend = "yend", color = "learner_id"), size = 1)
  # Plot Learner name
  p = p + geom_text(aes_string("xend", "yend", label = "learner_id", color = "learner_id",
                               hjust = "right"), vjust = -1)

  p = p + xlab("Average Rank")
  # Change appearance
  p = p + scale_x_continuous(breaks = c(0:max(obj$data$xend)))
  p = p + theme(axis.text.y = element_blank(),
                axis.ticks.y = element_blank(),
                axis.title.y = element_blank(),
                legend.position = "none",
                panel.background = element_blank(),
                panel.border = element_blank(),
                axis.line = element_line(size = 1),
                axis.line.y = element_blank(),
                panel.grid.major = element_blank(),
                plot.background = element_blank())

  # Write some values into shorter names as they are used numerous times.
  cd.x = obj$cd.info$x # nolint
  cd.y = obj$cd.info$y # nolint
  cd = obj$cd.info$cd

  # Add crit difference test (descriptive)
  p = p + annotate("text",
                   label = paste("Critical Difference =", round(cd, 2), sep = " "),
                   y = max(obj$data$yend) + 0.8, x = mean(obj$data$mean.rank))
  # Add bar (descriptive)
  p = p + annotate("segment",
                   x = mean(obj$data$mean.rank) - 0.5 * cd,
                   xend = mean(obj$data$mean.rank) + 0.5 * cd,
                   y = max(obj$data$yend) + 0.4,
                   yend = max(obj$data$yend) + 0.4,
                   size = 1.3, alpha = 0.9)

  # Plot the critical difference bars
  if (obj$cd.info$test == "bd") {
    cd.x = obj$data$mean.rank[obj$data$learner_id == obj$baseline] # nolint
    # Add horizontal bar around baseline
    p = p + annotate("segment", x = cd.x + cd,
                     xend = ifelse(cd.x - cd < 0, 0, cd.x - cd), y = cd.y, yend = cd.y,
                     alpha = 0.9, color = "dimgrey", size = 1.3)
    # Add interval limiting bar's
    p = p + annotate("segment", x = cd.x + cd, xend = cd.x + cd, y = cd.y - 0.05,
                     yend = cd.y + 0.05, color = "dimgrey", size = 1.3, alpha = 0.9)
    p = p + annotate("segment", x = cd.x - cd, xend = cd.x - cd, y = cd.y - 0.05,
                     yend = cd.y + 0.05, color = "dimgrey", size = 1.3, alpha = 0.9)
    # Add point at learner
    p = p + annotate("point", x = cd.x, y = cd.y, alpha = 0.6, color = "black")
  } else {
    nemenyi.data = obj$cd.info$nemenyi.data # nolint
    if (!(nrow(nemenyi.data) == 0L)) {
      # Add connecting bars
      p = p + geom_segment(aes_string("xstart", "y", xend = "xend", yend = "y"),
                           data = nemenyi.data, size = 1.3, color = "dimgrey", alpha = 0.9,
                           )
    } else {
      message("No connecting bars to plot!")
    }
  }

  return(p)
}
#' @title Plots for BenchmarkAggr
#'
#' @description
#' Generates plots for [BenchmarkAggr], all assume that there are multiple, independent, tasks.
#' Choices depending on the argument `type`:
#'
#' * `"mean"` (default): Assumes there are at least two independent tasks. Plots the sample mean
#' of the measure for all learners with error bars computed with the standard error of the mean.
#' * `"box"`: Boxplots for each learners calculated over all tasks for a given measure.
#' * `"cd"`: Critical difference plots (Demsar, 2006), uses the `$crit_differences` method from
#' [BenchmarkAggr]. If a baseline is selected for the Bonferroni-Dunn test, the critical difference
#' interval will be positioned around the baseline. If not, the best performing algorithm will be
#' chosen as baseline. Learners are drawn on the y-axis according to their average rank.
#' For `test = "nemenyi"` a bar is drawn, connecting all groups of not
#' significantly different learners. For `test = "bd"` an interval is drawn around the algorithm
#' selected as a baseline and any learner within this interval is not significantly different
#' from the baseline.
#' * `"fn"`: Plots post-hoc Friedman-Nemenyi by first calling `[BenchmarkAggr]$friedman_posthoc`
#' and plotting significant pairs in coloured squares and leaving non-significant pairs blank.
#'
#' @param obj [BenchmarkAggr]
#' @param type `(character(1))` \cr Type of plot, see description.
#' @param meas `(character(1))` \cr Measure to plot, should be in `obj$measures`, can be `NULL` if
#' only one measure is in `obj`.
#' @param level `(numeric(1))` \cr Confidence level for error bars for `type = "mean"`
#' @param p.value `(numeric(1))` \cr What value should be considered significant for
#' `type = "cd"` and `type = "fn"`>
#' @param ... `ANY` \cr Additional arguments passed to `[BenchmarkAggr]$crit_differences`.
#'
#' @references Janez Demsar, Statistical Comparisons of Classifiers over
#' Multiple Data Sets, JMLR, 2006
#'
#' @examples
#' library(mlr3)
#' library(mlr3learners)
#' set.seed(1)
#' task = tsks(c("iris", "sonar", "wine", "zoo"))
#' learns = lrns(c("classif.featureless", "classif.ranger", "classif.xgboost"))
#' bm = benchmark(benchmark_grid(task, learns, rsmp("cv", folds = 3)))
#' obj = as.BenchmarkAggr(bm)
#'
#' # mean and error bars
#' autoplot(obj, type = "mean")
#'
#' # critical differences
#' autoplot(obj, type = "cd")
#'
#' # post-hoc friedman-nemenyi
#' autoplot(obj, type = "fn")
#'
#' @export
autoplot.BenchmarkAggr = function(obj, type = c("mean", "box", "cd", "fn"), meas = NULL, level = 0.95, # nolint
                                  p.value = 0.05, ...) { # nolint

  type = match.arg(type)

  meas = .check_meas(obj, meas)

  if (type == "cd") {
    .plot_crittdiff(obj, meas, p.value, ...)
  } else if (type == "mean") {
    if (obj$ntasks < 2) {
      stop("At least two tasks required.")
    }
    loss = aggregate(as.formula(paste0(meas, " ~ learner_id")), obj$data, mean)
    se = aggregate(as.formula(paste0(meas, " ~ learner_id")), obj$data, sd)[, 2] / sqrt(obj$ntasks)
    loss$lower = loss[, meas] - se * qnorm(1 - (1 - level) / 2)
    loss$upper = loss[, meas] + se * qnorm(1 - (1 - level) / 2)
    ggplot(data = loss, aes_string(x = "learner_id", y = meas)) +
      geom_errorbar(aes(ymin = lower, ymax = upper),
                    width = .5) +
      geom_point()
  } else if (type == "fn") {

    p = tryCatch(obj$friedman_posthoc(meas)$p.value,
      warning = function(w) stop("Global Friedman test non-significant, try type = 'mean' instead.")) # nolint

    p = reshape2::melt(p)
    p$value[is.na(p$value)] = 1
    p$value = factor(ifelse(p$value < p.value, "0", "1"))
    ggplot(data = p, aes(x = Var1, y = Var2, fill = value)) +
      geom_tile(color = "black", size = 0.5) +
      scale_fill_manual(name = "col",
                          values = c("0" = "red", "1" = "white"),
                          labels = c("0", "1"),
                        aesthetics = c("colour", "fill")) +
      theme(axis.title = element_blank(),
            axis.text.y = element_text(angle = 45),
            axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 0.7),
            panel.grid = element_blank(),
            panel.background = element_rect(fill = "white"),
            legend.position = "none") +
      labs(title = paste0("Pairwise Nemenyi Post-Hoc of ", meas))
  } else if (type == "box") {
    ggplot(data = obj$data,
           aes_string(x = "learner_id", y = meas)) +
      geom_boxplot()
  }

}

#' @export
as.BenchmarkAggr = function(obj, ...) UseMethod("as.BenchmarkAggr", obj) # nolint
#' @export
as.BenchmarkAggr.default = function(obj, ...) BenchmarkAggr$new(obj) # nolint
#' @export
as.BenchmarkAggr.BenchmarkResult = function(obj, ...) BenchmarkAggr$new(obj$aggregate(...)) # nolint

.check_meas = function(obj, meas) {
  if (is.null(meas)) {
    if (obj$nmeas > 1) {
      stop("Multiple measures available but `meas` is NULL. Please specify a measure.")
    } else {
      meas = obj$measures
    }
  }

  return(meas)
}
